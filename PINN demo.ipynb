{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fe56297-b9ad-4956-a65d-610aef692621",
   "metadata": {},
   "source": [
    "# Physics-informed neural network (PINN) example with harmonic oscillator\n",
    "\n",
    "In this demo we will code a PINN from scratch in `PyTorch` and use it to solve simulation and inversion problems related to the damped harmonic oscillator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dee1bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5037a8-d534-4f24-aafb-7f712e41c881",
   "metadata": {},
   "source": [
    "## Function Approximation with PyTorch\n",
    "\n",
    "The objective of this tutorial is learn the basics of PyTorch.\n",
    "\n",
    "First, we cosider the task of learning a function, with two simple examples:\n",
    "\n",
    "Polynomial class (considered before in class):\n",
    "$$f(x) =2x-10x^5+15x^{10}, \\quad x\\in(0, 1)$$\n",
    "\n",
    "Damped oscillations (example for PINN):\n",
    "$$ f(x) = e^{-x/\\delta} \\sin(\\omega x), \\quad x\\in(0, 2 \\pi) $$\n",
    "\n",
    "by using feed-forward dense neural networks.\n",
    "\n",
    "Three Steps:\n",
    "1.  Dataset generation\n",
    "2.  Build the Pytorch Model\n",
    "3.  Pytorch training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a8579-b7b9-47fe-9525-429b4cadc5ec",
   "metadata": {},
   "source": [
    "### $\\mathbf{\\text{Step 1: Dataset Generation}}$\n",
    "\n",
    "Consider a probabilistic process that gives rise to labeled data $(x,y)$. The training data is generated by drawing samples from the equation\n",
    "$$\n",
    "    y_i= f(x_i) + \\eta_i,\n",
    "$$\n",
    "where $f(x_i)$ is some fixed function, and $\\eta_i$ is a Gaussian, uncorrelated noise variable such that\n",
    "$$\n",
    "\\langle \\eta_i \\rangle=0 \n",
    "$$\n",
    "$$\n",
    "\\langle \\eta_i \\eta_j \\rangle = \\delta_{ij} \\sigma\n",
    "$$\n",
    "We will refer to the $f(x_i)$ as the **true model** used to generate the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a85ebd04-33b6-4f61-8454-cf3c070efc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate two kinds of training data, we will work with the polynomial data first\n",
    "def generate_data_polynomial(num_points, noise_std):\n",
    "    # Generate random x values\n",
    "    max_x = 1\n",
    "    x_values = np.sort(max_x*np.random.random(num_points))\n",
    "    # Generate corresponding y values for polynomial with added noise\n",
    "    y_values =2*x_values-10*x_values**5+15*x_values**10 + np.random.normal(0, noise_std, num_points)\n",
    "    return x_values, y_values\n",
    "\n",
    "def generate_data_damped_oscillation(num_points, noise_std):   \n",
    "    x_values = np.sort(np.random.uniform(0, 2 * np.pi, num_points))\n",
    "    # generate damped oscillation \n",
    "    y_values = np.sin(4*x_values)*np.exp(-x_values/2.0) + np.random.normal(0, noise_std, num_points)\n",
    "    return x_values, y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7697c4d5-da74-41a9-9be9-76cdb06de4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100\n",
    "# You can adjust the noise level as needed\n",
    "x_train, y_train = generate_data_polynomial(num_points, noise_std=0.1)\n",
    "# Plot the generated data for polynomial\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    x_train, y_train,\n",
    "    label='Generated Data with Noise', color='blue', alpha=0.5)\n",
    "plt.plot(\n",
    "    x_train, 2*x_train-10*x_train**5+15*x_train**10,\n",
    "    label='True Polynomial', color='red', linestyle='--')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Generated Polynomial Data with Noise')\n",
    "plt.xlabel('X values')\n",
    "plt.ylabel('Y values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa36cb0-35e3-4b83-838d-6786b1a9f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and plot data for damped oscillations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97745ed0-847c-4cfb-ba8d-19ac858abb9d",
   "metadata": {},
   "source": [
    "$\\mathbf{\\text{Step 2: Model Definition}}$\n",
    "\n",
    "We use feedforward neural (also named as a multi-layer perceptron) network to approximate the function  $f(x)$\n",
    "Given an input $x \\in D \\subset R^n$, a feedforward neural network transforms it to an output $NN_\\theta(x)\\in R^m$, through a layer of units (neurons) which compose of either affine-linear maps between units (in successive layers) or scalar non-linear activation functions within units, resulting in the representation,\n",
    "\n",
    "$$NN_{\\theta}(x) = C_K \\circ A \\circ C_{K-1}\\ldots \\ldots \\ldots \\circ A\\circ C_2 \\circ A\\circ C_1(x).$$\n",
    "\n",
    "Here, $\\circ$ refers to the composition of functions and $A$ is a scalar (non-linear) activation function. For any $1 \\leq k \\leq K$, we define\n",
    "\n",
    "$$\n",
    "C_k z_k = W_k z_k + b_k, \\quad {\\rm for} ~ W_k \\in R^{d_{k+1} \\times d_k}, z_k \\in R^{d_k}, b_k \\in R^{d_{k+1}}.\n",
    "$$\n",
    "\n",
    "We also denote,\n",
    "$$\n",
    "\\theta = \\{W_k, b_k\\}, \\theta_W = \\{ W_k \\}\\quad \\forall~ 1 \\leq k \\leq K,\n",
    "$$\n",
    "to be the complete set of (tunable) weights for our network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b996fd8-a3af-4bc2-8e8c-deb200354a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 10)  # 1 input feature, 10 hidden units\n",
    "        self.activation = nn.SiLU()\n",
    "        self.fc2 = nn.Linear(10, 1)  # 10 hidden units, 1 output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7461e8-c007-4e9c-a584-6ff5485fe26b",
   "metadata": {},
   "source": [
    "$\\mathbf{\\text{Step 3: Model Training}}$\n",
    "\n",
    "The neural network $u_{\\theta}$ depends on the tuning parameter $\\theta \\in \\Theta$ of weights and biases. Within the standard paradigm of deep learning, one trains the network by finding tuning parameters $\\theta$ such that a suitable loss function $J(\\theta)$ is minimized.\n",
    "\n",
    "$${\\rm Find}~\\theta^{\\ast} \\in \\Theta:\\quad \\theta^{\\ast} = {\\rm arg}\\min\\limits_{\\theta \\in \\Theta} \\left( J(\\theta)\\right).$$\n",
    "\n",
    "The loss function, for instance, can be chosen as the mean square error between the neural network and the underlying target:\n",
    "\n",
    "$$ J(\\theta) = \\sum_{i}^n \\Big(u_i - u_\\theta(x_i)\\Big)^2$$\n",
    "\n",
    "The optimization process is realized with the gradient descent (or more precisely with variants of the gradient descent such as Adam or SGD).\n",
    "\n",
    "1. Compute the loss function over the batch $j$:\n",
    "$J_S(\\theta)=\\sum_{x_i \\in {S}_j}^n \\Big(u_i - u_\\theta(x_i)\\Big)^2$\n",
    "\n",
    "2. Compute the gradient of $J_S(\\theta)$ with respect to the network parameters:  $\\nabla_\\theta J_S(\\theta)$\n",
    "\n",
    "3. Update the parameters according to the chosen optimizer, for instance for minibatch stochastic gradient descent $\\theta_{k+1} = \\theta_{k} - \\eta \\nabla_\\theta J_S(\\theta_{k}) $ with $k=1,...,(n_{epoch} n_{batch})$ and $\\eta$ being the learning rate (argument $lr$ in the optimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88541164-a56d-4d4a-bcec-a3eebd34feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(optimizer_name, model):\n",
    "  if optimizer_name == \"ADAM\":\n",
    "      return optim.Adam(model.parameters())\n",
    "  elif optimizer_name == \"LBFGS\":\n",
    "      return optim.LBFGS(model.parameters())\n",
    "  raise ValueError('Chosen optimizer is unavailable.')\n",
    "\n",
    "\n",
    "def train(model, optimizer, criterion, inputs, targets):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    return closure().item()\n",
    "\n",
    "\n",
    "def evaluate(model, x_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x_test.view(-1, 1)).numpy()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cdf403-0660-4896-bd6d-206011de681f",
   "metadata": {},
   "source": [
    "#### Instantiate your model, define your loss function + optimizer, and perform the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230fec5-7b6e-42f5-a438-96c9ef7c1352",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = get_optimizer('ADAM', model)\n",
    "\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    # key step! Convert data to PyTorch tensors, a special data structure for Pytorch (similar to ndarrays for numpy)\n",
    "    inputs = torch.tensor(\n",
    "        x_train, dtype=torch.float32, requires_grad=True).view(-1, 1)\n",
    "    targets = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Training\n",
    "    loss = train(model, optimizer, criterion, inputs, targets)\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246cdd6a-a4ba-4b2b-868a-1858bf66499b",
   "metadata": {},
   "source": [
    "$\\mathbf{\\text{Step 4: Plotting Predictions}}$\n",
    "Final step, evaluate your predictions at some input locations and do some plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f8eaf-8c24-43e2-9a7a-0d10856b6904",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.linspace(0, 1., 100)\n",
    "y_pred = evaluate(model, x_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_train, y_train, label='Original Noisy Training Data', color='orange', alpha=1)\n",
    "plt.plot(x_test,2*x_test-10*x_test**5+15*x_test**10, label='Original True Polynomial', color='blue', alpha=1.0)\n",
    "plt.plot(x_test, y_pred, label='Learned Curve', color='red', alpha=1.0)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e17659-98ec-435c-9761-9adba7f83251",
   "metadata": {},
   "source": [
    "Interesting parameters to play with:  Size of training data and noise\n",
    "number of learning steps (epochs) and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115c077-695e-47f8-b12e-a18cb10f8dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e6981b5",
   "metadata": {},
   "source": [
    "### Your turn:\n",
    "Compare training, testing and predicted data for damped oscillator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10af7128-0ae8-4503-abdf-24cb2ed3e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.linspace(0, 1.2*2*np.pi, 100)\n",
    "y_pred = evaluate(model, x_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_train, y_train, label='Original Noisy Training Data', color='orange', alpha=1)\n",
    "plt.plot(x_test,np.sin(4*x_test)*np.exp(-x_test/2.0), label='Original Damped Oscillation', color='blue')\n",
    "plt.plot(x_test, y_pred, label='Learned Curve', color='red')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c24a9b-623e-494e-8b43-27685c31f976",
   "metadata": {},
   "source": [
    "## Physics-Informed Neural Network (PINN) Problem overview\n",
    "\n",
    "This example builds upon the blog post tutorial by Ben Moseley on PINNs: https://benmoseley.blog/my-research/so-what-is-a-physics-informed-neural-network/. \n",
    "\n",
    "Read the seminal PINN papers [here](https://ieeexplore.ieee.org/document/712178) and [here](https://www.sciencedirect.com/science/article/pii/S0021999118307125).\n",
    "\n",
    "In this demo we will code a PINN from scratch in  **PyTorch** and use it to solve simulation and inversion problems related to the damped harmonic oscillator.\n",
    "\n",
    "We are going to use a PINN to solve problems related to the **damped harmonic oscillator**:\n",
    "\n",
    "<img src=\"oscillator.gif\" width=\"500\">\n",
    "\n",
    "We are interested in modelling the displacement of the mass on a spring (green box) over time.\n",
    "\n",
    "This is a canonical physics problem, where the displacement, $u(t)$, of the oscillator as a function of time can be described by the following differential equation:\n",
    "\n",
    "$$\n",
    "m \\dfrac{d^2 u}{d t^2} + \\mu \\dfrac{d u}{d t} + ku = 0~,\n",
    "$$\n",
    "\n",
    "where $m$ is the mass of the oscillator, $\\mu$ is the coefficient of friction and $k$ is the spring constant.\n",
    "\n",
    "We will focus on solving the problem in the **under-damped state**, i.e. where the oscillation is slowly damped by friction (as displayed in the animation above). \n",
    "\n",
    "Mathematically, this occurs when:\n",
    "\n",
    "$$\n",
    "\\delta < \\omega_0~,~~~~~\\mathrm{where}~~\\delta = \\dfrac{\\mu}{2m}~,~\\omega_0 = \\sqrt{\\dfrac{k}{m}}~.\n",
    "$$\n",
    "\n",
    "Furthermore, we consider the following initial conditions of the system:\n",
    "\n",
    "$$\n",
    "u(t=0) = 1~~,~~\\dfrac{d u}{d t}(t=0) = 0~.\n",
    "$$\n",
    "\n",
    "For this particular case, the exact solution is known and given by:\n",
    "\n",
    "$$\n",
    "u(t) = e^{-\\delta t}(2 A \\cos(\\phi + \\omega t))~,~~~~~\\mathrm{with}~~\\omega=\\sqrt{\\omega_0^2 - \\delta^2}~.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "For a more detailed mathematical description of the harmonic oscillator, check out this blog post: https://beltoforion.de/en/harmonic_oscillator/.\n",
    "\n",
    "## Workflow overview\n",
    "\n",
    "There are **two scientific tasks** related to the harmonic oscillator we will use a PINN for:\n",
    "\n",
    ">First, we will **simulate** the system using a PINN, given its initial conditions.\n",
    "\n",
    ">Second, we will **invert** for underlying parameters of the system using a PINN, given some noisy observations of the oscillator's displacement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d8ed8e-39e8-410a-a406-491e25fbd375",
   "metadata": {},
   "source": [
    "## Initial setup\n",
    "\n",
    "First, we define a few helper functions.\n",
    "\n",
    "The constructor initializes the model with:\n",
    "\n",
    "N_INPUT: Number of input features\n",
    "\n",
    "N_OUTPUT: Number of output features\n",
    "\n",
    "N_HIDDEN: Number of hidden units per layer\n",
    "\n",
    "N_LAYERS: Total number of layers including input and output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ac19c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_solution(d, w0, t):\n",
    "    \"Defines the analytical solution to the under-damped harmonic oscillator problem above.\"\n",
    "    assert d < w0\n",
    "    w = np.sqrt(w0**2-d**2)\n",
    "    # this expression for the phase phi is from the initial condition: du/dt (t=0) = 0\n",
    "    phi = np.arctan(-d/w)\n",
    "    #this expression of A is from the initial condition: u(t=0) = 1:\n",
    "    A = 1/(2*np.cos(phi))\n",
    "    cos = torch.cos(phi+w*t)\n",
    "    exp = torch.exp(-d*t)\n",
    "    u = exp*2*A*cos\n",
    "    return u\n",
    "\n",
    "#This class FCN inherits from torch.nn.Module, the base class for all neural network models in PyTorch.\n",
    "class FCN(nn.Module):\n",
    "    \"Defines a fully-connected network in PyTorch\"\n",
    "    def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS):\n",
    "        super().__init__()\n",
    "        activation = nn.Tanh\n",
    "        # self.fcs is the input layer followed by a Tanh activation.\n",
    "        self.fcs = nn.Sequential(*[\n",
    "                        nn.Linear(N_INPUT, N_HIDDEN),\n",
    "                        activation()])\n",
    "        #self.fch is a sequence of (N_LAYERS - 1) identical blocks \n",
    "        # each containing a linear layer and a tanh activation\n",
    "        self.fch = nn.Sequential(*[\n",
    "                        nn.Sequential(*[\n",
    "                            nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "                            activation()]) for _ in range(N_LAYERS-1)])\n",
    "        #output layer maps hidden activation to output features\n",
    "        self.fce = nn.Linear(N_HIDDEN, N_OUTPUT)\n",
    "    def forward(self, x):\n",
    "        x = self.fcs(x)  # Apply input layer and activation\n",
    "        x = self.fch(x)  # Pass through hidden layers\n",
    "        x = self.fce(x)  #Apply output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52da0272-a28c-464f-a4f4-69fcf34ecd8e",
   "metadata": {},
   "source": [
    "## Task 1: train a PINN to simulate the system\n",
    "\n",
    "#### Task\n",
    "\n",
    "The first task is to use a PINN to **simulate** (solve) the system.\n",
    "\n",
    "Specifically, our inputs and outputs are:\n",
    "\n",
    "- Inputs: underlying differential equation and the initial conditions of the system\n",
    "- Outputs: estimate of the solution, $u(t)$\n",
    "\n",
    "#### Approach\n",
    "\n",
    "The PINN is trained to directly approximate the solution to the differential equation, i.e.\n",
    "\n",
    "$$\n",
    "N\\!N(t;\\theta) \\approx u(t)~,\n",
    "$$\n",
    "\n",
    "For this task, we use $\\delta=2$, $\\omega_0=20$, $m=1$, and try to learn the solution over the domain $t\\in [0,1]$.\n",
    "\n",
    "#### Loss function\n",
    "\n",
    "To simulate the system, the PINN is trained with the following loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta)= (N\\!N(0;\\theta) - 1)^2 + \\lambda_1 \\left(\\frac{d N\\!N}{dt}(0;\\theta) - 0\\right)^2 + \\frac{\\lambda_2}{N} \\sum^{N}_{i} \\left( \\left[ m\\frac{d^2}{dt^2} + \\mu \\frac{d}{dt} + k \\right] N\\!N(t_{i};\\theta)  \\right)^2\n",
    "$$\n",
    "\n",
    "where $\\lambda_1$ and $\\lambda_2$ are hyperparameters used for regularization of the training.\n",
    "\n",
    "The first two terms in the loss function represent the boundary loss, and tries to ensure that the solution learned by the PINN matches the initial conditions of the system, namely.\n",
    "The second term in the loss function is called the physics loss, and and tries to ensure that the PINN solution obeys the underlying differential equation at a set of training points \n",
    "sampled over the entire domain.\n",
    "\n",
    "#### Computing gradients\n",
    "\n",
    "To compute gradients of the neural network with respect to its inputs, we will use `torch.autograd.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da147e-67a7-4e2c-a98d-3d6e9d69d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# define a neural network to train  ( Fully-connected Network)\n",
    "pinn = FCN(1,1,32,3)\n",
    "\n",
    "# define boundary points, for the boundary loss\n",
    "t_boundary = torch.tensor(0.).view(-1,1).requires_grad_(True)# (1, 1)\n",
    "\n",
    "# define training points over the entire domain, for the physics loss\n",
    "t_physics = torch.linspace(0,1,30).view(-1,1).requires_grad_(True)# (30, 1)\n",
    "\n",
    "# train the PINN\n",
    "d, w0 = 2, 20\n",
    "mu, k = 2*d, w0**2\n",
    "t_test = torch.linspace(0,1,300).view(-1,1)\n",
    "u_exact = exact_solution(d, w0, t_test)\n",
    "#specify optimizer choice and learning rate\n",
    "optimiser = torch.optim.Adam(pinn.parameters(),lr=1e-3)\n",
    "for i in range(15001):\n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    # compute each term of the PINN loss function above\n",
    "    # using the following hyperparameters\n",
    "    lambda1, lambda2 = 1e-1, 1e-4\n",
    "    \n",
    "    # compute boundary loss\n",
    "    u = pinn(t_boundary)# (1, 1)\n",
    "    loss1 = (torch.squeeze(u) - 1)**2\n",
    "    dudt = torch.autograd.grad(u, t_boundary, torch.ones_like(u), create_graph=True)[0]# (1, 1)\n",
    "    loss2 = (torch.squeeze(dudt) - 0)**2\n",
    "    \n",
    "    # compute physics loss\n",
    "    u = pinn(t_physics)# (30, 1)\n",
    "    dudt = torch.autograd.grad(u, t_physics, torch.ones_like(u), create_graph=True)[0]# (30, 1)\n",
    "    d2udt2 = torch.autograd.grad(dudt, t_physics, torch.ones_like(dudt), create_graph=True)[0]# (30, 1)\n",
    "    loss3 = torch.mean((d2udt2 + mu*dudt + k*u)**2)\n",
    "    \n",
    "    # backpropagate joint loss, take optimiser step\n",
    "    loss = loss1 + lambda1*loss2 + lambda2*loss3\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "    # plot the result as training progresses\n",
    "    if i % 5000 == 0: \n",
    "        #print(u.abs().mean().item(), dudt.abs().mean().item(), d2udt2.abs().mean().item())\n",
    "        u = pinn(t_test).detach()\n",
    "        plt.figure(figsize=(6,2.5))\n",
    "        plt.scatter(t_physics.detach()[:,0], \n",
    "                    torch.zeros_like(t_physics)[:,0], s=20, lw=0, color=\"tab:green\", alpha=0.6)\n",
    "        plt.scatter(t_boundary.detach()[:,0], \n",
    "                    torch.zeros_like(t_boundary)[:,0], s=20, lw=0, color=\"tab:red\", alpha=0.6)\n",
    "        plt.plot(t_test[:,0], u_exact[:,0], label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n",
    "        plt.plot(t_test[:,0], u[:,0], label=\"PINN solution\", color=\"tab:green\")\n",
    "        plt.title(f\"Training step {i}\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae9e26-9f51-4ef9-9b46-b710c791b8c8",
   "metadata": {},
   "source": [
    "## Task 2: train a PINN to invert for underlying parameters\n",
    "\n",
    "#### Task\n",
    "\n",
    "The second task is to use a PINN to **invert** for underlying parameters.\n",
    "\n",
    "Specifically, our inputs and outputs are:\n",
    "\n",
    "- Inputs: noisy observations of the oscillator's displacement, $u_{\\mathrm{obs}}$\n",
    "- Outputs: estimate $\\mu$, the coefficient of friction\n",
    "\n",
    "#### Approach\n",
    "\n",
    "Similar to above, the PINN is trained to directly approximate the solution to the differential equation, i.e.\n",
    "\n",
    "$$\n",
    "N\\!N(t;\\theta) \\approx u(t)~,\n",
    "$$\n",
    "\n",
    "However here we assume $\\mu$ is **not known** and we treat it as an additional **learnable parameter** when training the PINN.\n",
    "\n",
    "#### Loss function\n",
    "\n",
    "The PINN is trained with the loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\mu)= \\frac{1}{N} \\sum^{N}_{i} \\left( \\left[ m\\frac{d^2}{dt^2} + \\mu \\frac{d}{dt} + k \\right] N\\!N(t_{i};\\theta)  \\right)^2 + \\frac{\\lambda}{M} \\sum^{M}_{j} \\left( N\\!N(t_{j};\\theta) - u_{\\mathrm{obs}}(t_{j}) \\right)^2\n",
    "$$\n",
    "\n",
    "There are two terms in the loss function here. The first is the physics loss, formed in the same way as above, which ensures the solution learned by the PINN is consistent with the know physics.\n",
    "\n",
    "The second term is called the data loss, and makes sure that the solution learned by the PINN fits the (potentially noisy) observations of the solution that are available.\n",
    "\n",
    "Note, we have removed the boundary loss terms, as we do not know these (i.e., we are only given the observed measurements of the system).\n",
    "\n",
    "In this set up, the PINN parameters $\\theta$ and $\\mu$ are jointly learned during optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5871be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create some noisy observational data\n",
    "torch.manual_seed(123)\n",
    "d, w0 = 2, 20\n",
    "print(f\"True value of mu: {2*d}\")\n",
    "t_obs = torch.rand(40).view(-1,1)\n",
    "u_obs = exact_solution(d, w0, t_obs) + 0.04*torch.randn_like(t_obs)\n",
    "t_test = torch.linspace(0,1,300).view(-1,1)\n",
    "u_exact = exact_solution(d, w0, t_test)\n",
    "\n",
    "plt.figure(figsize=(6,2.5))\n",
    "plt.title(\"Noisy observational data\")\n",
    "plt.scatter(t_obs[:,0], u_obs[:,0])\n",
    "plt.plot(t_test[:,0], u_exact[:,0], label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df8a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# define a neural network to train\n",
    "pinn = FCN(1,1,32,3)\n",
    "\n",
    "# define training points over the entire domain, for the physics loss\n",
    "t_physics = torch.linspace(0,1,30).view(-1,1).requires_grad_(True)# (30, 1)\n",
    "\n",
    "# train the PINN\n",
    "d, w0 = 2, 20\n",
    "_, k = 2*d, w0**2\n",
    "t_test = torch.linspace(0,1,300).view(-1,1)\n",
    "u_exact = exact_solution(d, w0, t_test)\n",
    "\n",
    "# treat mu as a learnable parameter, add it to optimiser\n",
    "mu = torch.nn.Parameter(torch.zeros(1, requires_grad=True))\n",
    "optimiser = torch.optim.Adam(list(pinn.parameters())+[mu],lr=1e-3)\n",
    "mus = []\n",
    "for i in range(15001):\n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    # compute each term of the PINN loss function above\n",
    "    # using the following hyperparameters\n",
    "    lambda1 = 1e4\n",
    "    \n",
    "    # compute physics loss\n",
    "    u = pinn(t_physics)# (30, 1)\n",
    "    dudt = torch.autograd.grad(u, t_physics, torch.ones_like(u), create_graph=True)[0]# (30, 1)\n",
    "    d2udt2 = torch.autograd.grad(dudt, t_physics, torch.ones_like(dudt), create_graph=True)[0]# (30, 1)\n",
    "    loss1 = torch.mean((d2udt2 + mu*dudt + k*u)**2)\n",
    "    \n",
    "    # compute data loss\n",
    "    u = pinn(t_obs)\n",
    "    loss2 = torch.mean((u - u_obs)**2)\n",
    "    \n",
    "    # backpropagate joint loss, take optimiser step\n",
    "    loss = loss1 + lambda1*loss2\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "    # record mu value\n",
    "    mus.append(mu.item())\n",
    "    \n",
    "    # plot the result as training progresses\n",
    "    if i % 5000 == 0: \n",
    "        u = pinn(t_test).detach()\n",
    "        plt.figure(figsize=(12,2.5))\n",
    "        \n",
    "        plt.subplot(1,2,1)\n",
    "        plt.scatter(t_obs[:,0], u_obs[:,0], label=\"Noisy observations\", alpha=0.6, color=\"tab:blue\")\n",
    "        plt.plot(t_test[:,0], u[:,0], label=\"PINN solution\", color=\"tab:green\")\n",
    "        plt.title(f\"Training step {i}\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1,2,2)\n",
    "        plt.title(\"$\\mu$\")\n",
    "        plt.plot(mus, label=\"PINN estimate\", color=\"tab:green\")\n",
    "        plt.hlines(2*d, 0, len(mus), label=\"True value\", color=\"tab:grey\")\n",
    "        plt.xlabel(\"Training step\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf45f9f-a90b-4f14-8e59-f12bdace7f1a",
   "metadata": {},
   "source": [
    "Notice how the PINN estimate of $\\mu$ finally converges to its true value!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e69c626-a46d-4dd7-a4e9-ad173e7da9f4",
   "metadata": {},
   "source": [
    "## Suggested Extensions\n",
    "\n",
    "PINNs have been extended and improved in many ways since they have been proposed. Some things to try are:\n",
    "\n",
    "- Try extending them to higher dimensions (e.g. 2D and 3D simulations)\n",
    "- See how far you can push the inversion task: can you discover $m$, $\\mu$ and $k$ simultaneously (and therefore, discover the entire underlying equation?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df65092-74fe-4912-add3-a2711bf6523c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
